{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "page = requests.get(url=URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "headers = ['h1', 'h2', 'h3', 'h4', 'h5']\n",
    "headings = soup.find_all(headers)\n",
    "for heads in headings:\n",
    "    print(heads.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLS = [\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\" ,\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\"]\n",
    "\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "lst = []\n",
    "for url in URLS:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    mainHead = soup.findAll(\"div\", class_ = \"lister-item mode-advanced\")\n",
    "    for i in mainHead:\n",
    "        header = i.find(\"h3\", \"lister-item-header\")\n",
    "        name = header.find('a')\n",
    "        rating = i.find(\"div\", \"inline-block ratings-imdb-rating\").attrs.get(\"data-value\", None)\n",
    "        year = i.find(\"span\", \"lister-item-year text-muted unbold\")\n",
    "        lst.append([name.contents[0], rating, year.contents[0][1:5]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Rating\", \"Year\"])\n",
    "df.to_csv('IMDB.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41238820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "page = requests.get(url=URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "divs_presidents = soup.find_all(\"div\", class_ = \"presidentListing\")\n",
    "for content in divs_presidents:\n",
    "    presidents = content.find_all('h3')\n",
    "    for president in presidents:\n",
    "        print(president.contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'user-agent': \"mozilla/5.0 (macintosh; intel mac os x 10_15_7) applewebkit/537.36 (khtml, like gecko) chrome/96.0.4664.110 safari/537.36\"}\n",
    "URL = \"\"\n",
    "lst = []\n",
    "names = []\n",
    "\n",
    "choice = int(input(\"1. Men's Cricket ODI Rantings\\n2. Top 10 ODI Batsmen Records\\n3. Top 10 ODI Bowler's Records: \"))\n",
    "lst = []\n",
    "match choice:\n",
    "    case 1:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "        names = [\"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "    case 2:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "        names = [\"Player\", \"Team\", \"Rating\", \"Career Best Rating\"]\n",
    "    case 3:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "        names = [\"Player\", \"Team\", \"Rating\", \"Career Best Rating\"]\n",
    "\n",
    "    case _:\n",
    "        print(\"Invalid Input! Try Again\")\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "table_rows = table.find_all('tr')[1:]\n",
    "\n",
    "for tr in table_rows[:10]:\n",
    "    td = tr.find_all('td')\n",
    "    row = [i.text.strip() for i in td if str(i)]\n",
    "    lst.append(row)\n",
    "\n",
    "for i in lst:\n",
    "    try:\n",
    "        i.pop(0)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(lst,columns=names)\n",
    "df.to_csv('ICC.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf626ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.cnbc.com/world/?region=world:\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "lst = []\n",
    "\n",
    "# Headline News\n",
    "headline = soup.find(\"h2\", class_ = \"FeaturedCard-packagedCardTitle\").find(\"a\")\n",
    "lst.append([\"0 hours ago\",headline.contents[0], headline['href']])\n",
    "\n",
    "# Latest News\n",
    "count = 1\n",
    "latest_header = soup.find_all(\"div\", class_ = \"LatestNews-headlineWrapper\")\n",
    "for headings in latest_header:\n",
    "    time = headings.find(\"time\", \"LatestNews-timestamp\").contents[0]\n",
    "    heading = headings.find('a')\n",
    "    lst.append([time,heading.contents[0], heading['href']])\n",
    "\n",
    "# Preprocessing\n",
    "df = pd.DataFrame(lst, columns=[\"Time\", \"Article\", \"Link\"])\n",
    "drop_lst = []\n",
    "count = 0\n",
    "for i in df[\"Link\"]:\n",
    "    if i[0] != 'h':\n",
    "        drop_lst.append(count)\n",
    "    count += 1\n",
    "\n",
    "new_df = df.drop(df.index[drop_lst])\n",
    "new_df\n",
    "new_df.to_csv('CNBC.csv')\n",
    "print(new_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fe13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "list_of_articles = soup.find('ul', class_ = \"sc-9zxyh7-0 cMKaMj\")\n",
    "lst = []\n",
    "\n",
    "for article in list_of_articles.find_all('li'):\n",
    "    name = article.find('a')\n",
    "    authors = article.find('span', 'sc-1w3fpd7-0 dnCnAO').contents[0]\n",
    "    date = article.find('span', 'sc-1thf9ly-2 dvggWt')\n",
    "    link = name['href']\n",
    "    lst.append([name.find('h2').contents[0],\n",
    "                authors,\n",
    "                date.find('span').text,\n",
    "                link])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Authors\", \"Date of Publication\", \"Link\"])\n",
    "df.to_csv('AIArticles.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.dineout.co.in/bangalore-restaurants/welcome-back\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "restaurant_main_div = soup.find(\"div\", class_=\"restnt-card-wrap-new\")\n",
    "lst = []\n",
    "for restaurant in restaurant_main_div.find_all(\"div\", \"restnt-card restaurant\"):\n",
    "    name = restaurant.find('a', class_=\"restnt-name ellipsis\").contents[0]\n",
    "    location = restaurant.find('div', \"restnt-loc ellipsis\").find_all('a')\n",
    "    cuisine = restaurant.find(\"div\", \"detail-info\").find_all('a')\n",
    "    try:\n",
    "        ratings = restaurant.find('div', \"restnt-rating rating-4\").contents[0]\n",
    "    except AttributeError:\n",
    "        ratings = restaurant.find('div', \"restnt-rating rating-5\").contents[0]\n",
    "    image_url = restaurant.find('img', \"no-img\")[\"data-src\"]\n",
    "    loc_list, food_list = [loco.contents[0] for loco in location], [foo.contents[0] for foo in cuisine]\n",
    "    loc, food = \",\".join(loc_list), \",\".join(food_list)\n",
    "    lst.append([name, food, loc, ratings, image_url])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Cuisine\", \"Location\", \"Ratings\", \"Image URL\"])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ab571-d61e-46ea-96da-2c80e0b0155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://scholar.google.com/citations?view_op=top_venues&h1=en\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "table_rows = soup.find('table', class_=\"gsc_mp_table\").find_all('tr')\n",
    "lst = []\n",
    "for table in table_rows[1:]:\n",
    "    rank = table.find('td', \"gsc_mvt_p\").contents[0]\n",
    "    publication = table.find('td', \"gsc_mvt_t\").contents[0]\n",
    "    h5 = table.find_all(\"td\", \"gsc_mvt_n\")\n",
    "    h5_index = h5[0].find('a').contents[0]\n",
    "    h5_median = h5[1].find('span').contents[0]\n",
    "    lst.append([rank, publication, h5_index, h5_median])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Rank\", \"Publicaiton\", \"h5-index\", \"h5-median\"])\n",
    "df.to_csv('GoogleScholar.csv')\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
