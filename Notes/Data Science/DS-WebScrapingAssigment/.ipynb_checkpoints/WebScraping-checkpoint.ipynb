{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654ab98a-a9f3-42cb-bcd2-f643eb8e7797",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2849bf-36a5-43c4-9049-1d372404014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "page = requests.get(url=URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "headers = ['h1', 'h2', 'h3', 'h4', 'h5']\n",
    "headings = soup.find_all(headers)\n",
    "for heads in headings:\n",
    "    print(heads.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710d240-3f71-497e-9863-f1070c06e593",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a2bd08-f8ea-4194-968e-60dbecd089c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Name Rating  Year\n",
       "0                       The Shawshank Redemption    9.3  1994\n",
       "1                                  The Godfather    9.2  1972\n",
       "2                                The Dark Knight      9  2008\n",
       "3                               Schindler's List      9  1993\n",
       "4                          The Godfather Part II      9  1974\n",
       "5                                   12 Angry Men      9  1957\n",
       "6  The Lord of the Rings: The Return of the King      9  2003\n",
       "7                                   Pulp Fiction    8.9  1994\n",
       "8                                     Fight Club    8.8  1999\n",
       "9                                      Inception    8.8  2010"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLS = [\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\" ,\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\"]\n",
    "\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "lst = []\n",
    "for url in URLS:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    mainHead = soup.findAll(\"div\", class_ = \"lister-item mode-advanced\")\n",
    "    for i in mainHead:\n",
    "        header = i.find(\"h3\", \"lister-item-header\")\n",
    "        name = header.find('a')\n",
    "        rating = i.find(\"div\", \"inline-block ratings-imdb-rating\").attrs.get(\"data-value\", None)\n",
    "        year = i.find(\"span\", \"lister-item-year text-muted unbold\")\n",
    "        lst.append([name.contents[0], rating, year.contents[0][1:5]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Rating\", \"Year\"])\n",
    "df.to_csv('IMDB.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84f486-4124-48ce-8c6a-90c0495f211f",
   "metadata": {},
   "source": [
    "# Presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83322062-6618-4052-aa3e-a9d79518318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shri Ram Nath Kovind (birth - 1945)\n",
      "Shri Pranab Mukherjee (1935-2020)\n",
      "Smt Pratibha Devisingh Patil (birth - 1934)\n",
      "DR. A.P.J. Abdul Kalam (1931-2015)\n",
      "Shri K. R. Narayanan (1920 - 2005)\n",
      "Dr Shankar Dayal Sharma (1918-1999)\n",
      "Shri R Venkataraman (1910-2009)\n",
      "Giani Zail Singh (1916-1994)\n",
      "Shri Neelam Sanjiva Reddy (1913-1996)\n",
      "Dr. Fakhruddin Ali Ahmed (1905-1977)\n",
      "Shri Varahagiri Venkata Giri (1894-1980)\n",
      "Dr. Zakir Husain (1897-1969)\n",
      "Dr. Sarvepalli Radhakrishnan (1888-1975)\n",
      "Dr. Rajendra Prasad (1884-1963) \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "page = requests.get(url=URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "divs_presidents = soup.find_all(\"div\", class_ = \"presidentListing\")\n",
    "for content in divs_presidents:\n",
    "    presidents = content.find_all('h3')\n",
    "    for president in presidents:\n",
    "        print(president.contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6c27d-c706-481d-8082-707b6e59d300",
   "metadata": {},
   "source": [
    "# ICC Cricket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2ba1fb-9a04-4278-bd1b-5508615e68bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1. Men's Cricket ODI Rantings\n",
      "2. Top 10 ODI Batsmen Records\n",
      "3. Top 10 ODI Bowler's Records:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Player Team Rating             Career Best Rating\n",
      "0             Babar Azam  PAK    887  898 v West Indies, 10/06/2022\n",
      "1  Rassie van der Dussen   SA    777      796 v England, 19/07/2022\n",
      "2            Imam-ul-Haq  PAK    740  815 v West Indies, 12/06/2022\n",
      "3        Quinton de Kock   SA    740    813 v Sri Lanka, 10/03/2019\n",
      "4           Shubman Gill  IND    733    735 v Australia, 17/03/2023\n",
      "5           David Warner  AUS    732     880 v Pakistan, 26/01/2017\n",
      "6            Steve Smith  AUS    714     752 v Pakistan, 22/01/2017\n",
      "7            Virat Kohli  IND    714      911 v England, 12/07/2018\n",
      "8           Rohit Sharma  IND    704    885 v Sri Lanka, 06/07/2019\n",
      "9        Kane Williamson   NZ    700        799 v India, 09/07/2019\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'user-agent': \"mozilla/5.0 (macintosh; intel mac os x 10_15_7) applewebkit/537.36 (khtml, like gecko) chrome/96.0.4664.110 safari/537.36\"}\n",
    "URL = \"\"\n",
    "lst = []\n",
    "names = []\n",
    "\n",
    "choice = int(input(\"1. Men's Cricket ODI Rantings\\n2. Top 10 ODI Batsmen Records\\n3. Top 10 ODI Bowler's Records: \"))\n",
    "lst = []\n",
    "match choice:\n",
    "    case 1:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "        names = [\"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "    case 2:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "        names = [\"Player\", \"Team\", \"Rating\", \"Career Best Rating\"]\n",
    "    case 3:\n",
    "        URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "        names = [\"Player\", \"Team\", \"Rating\", \"Career Best Rating\"]\n",
    "\n",
    "    case _:\n",
    "        print(\"Invalid Input! Try Again\")\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "table_rows = table.find_all('tr')[1:]\n",
    "\n",
    "for tr in table_rows[:10]:\n",
    "    td = tr.find_all('td')\n",
    "    row = [i.text.strip() for i in td if str(i)]\n",
    "    lst.append(row)\n",
    "\n",
    "for i in lst:\n",
    "    try:\n",
    "        i.pop(0)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(lst,columns=names)\n",
    "df.to_csv('ICC.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844f335-b43c-427b-9f6d-31a1d95f3010",
   "metadata": {},
   "source": [
    "# CNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1cd7fc-5835-469f-85f6-22399c426702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Time                                            Article  \\\n",
      "0   0 hours ago  Alibaba shares soar 15% in Hong Kong on news o...   \n",
      "1    20 Min Ago  U.S.-China tech rivalry will continue to put C...   \n",
      "2    27 Min Ago  Asia's \"best restaurants\" list is out — and on...   \n",
      "3   2 Hours Ago  Alibaba shares soar 15% in Hong Kong on news o...   \n",
      "7   4 Hours Ago  Cramer: Rallies from McCormick and PVH could b...   \n",
      "8   4 Hours Ago  CNBC Daily Open: After SVB collapse, stricter ...   \n",
      "9   4 Hours Ago  Hong Kong shares jump over 2% as Alibaba surge...   \n",
      "10  4 Hours Ago  Jamie Dimon is being deposed over JPMorgan Cha...   \n",
      "12  6 Hours Ago  Google's failure to preserve employee chats in...   \n",
      "13  6 Hours Ago  Stock futures rise slightly after Nasdaq falls...   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2023/03/29/alibaba-hong-k...  \n",
      "1   https://www.cnbc.com/2023/03/29/us-china-tech-...  \n",
      "2   https://www.cnbc.com/2023/03/29/best-restauran...  \n",
      "3   https://www.cnbc.com/2023/03/29/alibaba-hong-k...  \n",
      "7   https://www.cnbc.com/2023/03/28/cramer-rallies...  \n",
      "8   https://www.cnbc.com/2023/03/29/stock-markets-...  \n",
      "9   https://www.cnbc.com/2023/03/29/asia-markets-m...  \n",
      "10  https://www.cnbc.com/2023/03/28/jamie-dimon-is...  \n",
      "12  https://www.cnbc.com/2023/03/28/google-not-pre...  \n",
      "13  https://www.cnbc.com/2023/03/28/stock-market-t...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.cnbc.com/world/?region=world:\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "lst = []\n",
    "\n",
    "# Headline News\n",
    "headline = soup.find(\"h2\", class_ = \"FeaturedCard-packagedCardTitle\").find(\"a\")\n",
    "lst.append([\"0 hours ago\",headline.contents[0], headline['href']])\n",
    "\n",
    "# Latest News\n",
    "count = 1\n",
    "latest_header = soup.find_all(\"div\", class_ = \"LatestNews-headlineWrapper\")\n",
    "for headings in latest_header:\n",
    "    time = headings.find(\"time\", \"LatestNews-timestamp\").contents[0]\n",
    "    heading = headings.find('a')\n",
    "    lst.append([time,heading.contents[0], heading['href']])\n",
    "\n",
    "# Preprocessing\n",
    "df = pd.DataFrame(lst, columns=[\"Time\", \"Article\", \"Link\"])\n",
    "drop_lst = []\n",
    "count = 0\n",
    "for i in df[\"Link\"]:\n",
    "    if i[0] != 'h':\n",
    "        drop_lst.append(count)\n",
    "    count += 1\n",
    "\n",
    "new_df = df.drop(df.index[drop_lst])\n",
    "new_df\n",
    "new_df.to_csv('CNBC.csv')\n",
    "print(new_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec78c71-7eb1-4c52-aa46-70e0e6fc83b4",
   "metadata": {},
   "source": [
    "# AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66946280-f76d-45b3-90f6-00844f82ef89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Date of Publication</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  \\\n",
       "0                                   Reward is enough   \n",
       "1                          Making sense of raw input   \n",
       "2  Law and logic: A review from an argumentation ...   \n",
       "3             Creativity and artificial intelligence   \n",
       "4  Artificial cognition for social human–robot in...   \n",
       "5  Explanation in artificial intelligence: Insigh...   \n",
       "6                      Making sense of sensory input   \n",
       "7  Conflict-based search for optimal multi-agent ...   \n",
       "8  Between MDPs and semi-MDPs: A framework for te...   \n",
       "9  The Hanabi challenge: A new frontier for AI re...   \n",
       "\n",
       "                                             Authors Date of Publication  \\\n",
       "0  Silver, David, Singh, Satinder, Precup, Doina,...        October 2021   \n",
       "1          Evans, Richard, Bošnjak, Matko and 5 more        October 2021   \n",
       "2                  Prakken, Henry, Sartor, Giovanni         October 2015   \n",
       "3                                Boden, Margaret A.          August 1998   \n",
       "4    Lemaignan, Séverin, Warnier, Mathieu and 3 more           June 2017   \n",
       "5                                       Miller, Tim        February 2019   \n",
       "6  Evans, Richard, Hernández-Orallo, José and 3 more          April 2021   \n",
       "7  Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...       February 2015   \n",
       "8  Sutton, Richard S., Precup, Doina, Singh, Sati...         August 1999   \n",
       "9        Bard, Nolan, Foerster, Jakob N. and 13 more          March 2020   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.sciencedirect.com/science/article/...  \n",
       "1  https://www.sciencedirect.com/science/article/...  \n",
       "2  https://www.sciencedirect.com/science/article/...  \n",
       "3  https://www.sciencedirect.com/science/article/...  \n",
       "4  https://www.sciencedirect.com/science/article/...  \n",
       "5  https://www.sciencedirect.com/science/article/...  \n",
       "6  https://www.sciencedirect.com/science/article/...  \n",
       "7  https://www.sciencedirect.com/science/article/...  \n",
       "8  https://www.sciencedirect.com/science/article/...  \n",
       "9  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "list_of_articles = soup.find('ul', class_ = \"sc-9zxyh7-0 cMKaMj\")\n",
    "lst = []\n",
    "\n",
    "for article in list_of_articles.find_all('li'):\n",
    "    name = article.find('a')\n",
    "    authors = article.find('span', 'sc-1w3fpd7-0 dnCnAO').contents[0]\n",
    "    date = article.find('span', 'sc-1thf9ly-2 dvggWt')\n",
    "    link = name['href']\n",
    "    lst.append([name.find('h2').contents[0],\n",
    "                authors,\n",
    "                date.find('span').text,\n",
    "                link])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Authors\", \"Date of Publication\", \"Link\"])\n",
    "df.to_csv('AIArticles.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930c036-e3c8-48f8-ab38-77c62d5e1924",
   "metadata": {},
   "source": [
    "# Dineout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1d9026-1a4f-4ccd-82ab-e967ec2b717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name                                 Cuisine  \\\n",
      "0  The Bier Library Brewery & Kitchen    Continental,Finger Food,North Indian   \n",
      "1                        Uru Brewpark  North Indian,Italian,Continental,Asian   \n",
      "2                      Hard Rock Cafe        Continental,American,Finger Food   \n",
      "3                  The Bangalore Cafe      Continental,North Indian,Fast Food   \n",
      "4                             Toscano                                 Italian   \n",
      "5                             Sanchez                                 Mexican   \n",
      "6                         Cafe Azzure    Continental,Italian,Desserts,Bengali   \n",
      "7                       Spice Terrace                    North Indian,Mughlai   \n",
      "8                          Biergarten                    Continental,European   \n",
      "9                           Cafe Noir                      Continental,French   \n",
      "\n",
      "                                            Location Ratings  \\\n",
      "0                        Koramangala,South Bangalore     4.4   \n",
      "1                           JP Nagar,South Bangalore     4.4   \n",
      "2                   St. Marks Road,Central Bangalore     4.4   \n",
      "3                     Shanti Nagar,Central Bangalore     4.3   \n",
      "4      UB City,,Vittal Mallya Road,Central Bangalore     4.5   \n",
      "5      UB City,,Vittal Mallya Road,Central Bangalore     4.3   \n",
      "6                          MG Road,Central Bangalore     4.3   \n",
      "7  JW Marriott Hotel,,Vittal Mallya Road,Central ...     4.2   \n",
      "8                        Koramangala,South Bangalore     4.4   \n",
      "9      UB City,,Vittal Mallya Road,Central Bangalore     4.4   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.dineout.co.in/bangalore-restaurants/welcome-back\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "restaurant_main_div = soup.find(\"div\", class_=\"restnt-card-wrap-new\")\n",
    "lst = []\n",
    "for restaurant in restaurant_main_div.find_all(\"div\", \"restnt-card restaurant\"):\n",
    "    name = restaurant.find('a', class_=\"restnt-name ellipsis\").contents[0]\n",
    "    location = restaurant.find('div', \"restnt-loc ellipsis\").find_all('a')\n",
    "    cuisine = restaurant.find(\"div\", \"detail-info\").find_all('a')\n",
    "    try:\n",
    "        ratings = restaurant.find('div', \"restnt-rating rating-4\").contents[0]\n",
    "    except AttributeError:\n",
    "        ratings = restaurant.find('div', \"restnt-rating rating-5\").contents[0]\n",
    "    image_url = restaurant.find('img', \"no-img\")[\"data-src\"]\n",
    "    loc_list, food_list = [loco.contents[0] for loco in location], [foo.contents[0] for foo in cuisine]\n",
    "    loc, food = \",\".join(loc_list), \",\".join(food_list)\n",
    "    lst.append([name, food, loc, ratings, image_url])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Name\", \"Cuisine\", \"Location\", \"Ratings\", \"Image URL\"])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d490e8c-e5df-48fe-8e3f-fa1839aafc35",
   "metadata": {},
   "source": [
    "# Google Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ab571-d61e-46ea-96da-2c80e0b0155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://scholar.google.com/citations?view_op=top_venues&h1=en\"\n",
    "header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "table_rows = soup.find('table', class_=\"gsc_mp_table\").find_all('tr')\n",
    "lst = []\n",
    "for table in table_rows[1:]:\n",
    "    rank = table.find('td', \"gsc_mvt_p\").contents[0]\n",
    "    publication = table.find('td', \"gsc_mvt_t\").contents[0]\n",
    "    h5 = table.find_all(\"td\", \"gsc_mvt_n\")\n",
    "    h5_index = h5[0].find('a').contents[0]\n",
    "    h5_median = h5[1].find('span').contents[0]\n",
    "    lst.append([rank, publication, h5_index, h5_median])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=[\"Rank\", \"Publicaiton\", \"h5-index\", \"h5-median\"])\n",
    "df.to_csv('GoogleScholar.csv')\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
